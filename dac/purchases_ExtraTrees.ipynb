{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "imposed-routine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-reservoir",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "auburn-praise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13972, 28) (4069, 28) (13972,) (4069,)\n"
     ]
    }
   ],
   "source": [
    "with open('X_train.npy', 'rb') as f:\n",
    "    X_train = np.load(f)\n",
    "with open('X_test.npy', 'rb') as f:\n",
    "    X_test = np.load(f)\n",
    "with open('y_train.npy', 'rb') as f:\n",
    "    y_train = np.load(f)\n",
    "with open('y_test.npy', 'rb') as f:\n",
    "    y_test = np.load(f)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interracial-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_train2.npy', 'rb') as f:\n",
    "    X_train2 = np.load(f)\n",
    "with open('X_test2.npy', 'rb') as f:\n",
    "    X_test2 = np.load(f)\n",
    "# with open('X_train3.npy', 'rb') as f:\n",
    "#     X_train3 = np.load(f)\n",
    "# with open('X_test3.npy', 'rb') as f:\n",
    "#     X_test3 = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "planned-riverside",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Administrative',\n",
    " 'Administrative_Duration',\n",
    " 'Informational',\n",
    " 'Informational_Duration',\n",
    " 'ProductRelated',\n",
    " 'ProductRelated_Duration',\n",
    " 'BounceRates',\n",
    " 'ExitRates',\n",
    " 'PageValues',\n",
    " 'SpecialDay',\n",
    " 'Month_Aug',\n",
    " 'Month_Dec',\n",
    " 'Month_Feb',\n",
    " 'Month_Jul',\n",
    " 'Month_June',\n",
    " 'Month_Mar',\n",
    " 'Month_May',\n",
    " 'Month_Nov',\n",
    " 'Month_Oct',\n",
    " 'Month_Sep',\n",
    " 'VisitorType_New_Visitor',\n",
    " 'VisitorType_Other',\n",
    " 'VisitorType_Returning_Visitor',\n",
    " 'Weekend_True',\n",
    " 'OperatingSystems',\n",
    " 'Browser',\n",
    " 'Region',\n",
    " 'TrafficType']\n",
    "\n",
    "feature_names = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "quiet-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train, columns=feature_names)\n",
    "X_test = pd.DataFrame(X_test, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "medieval-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_DF = pd.concat([X_train,X_test])\n",
    "y = np.concatenate((y_train,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-cotton",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "heard-packet",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "upset-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    \"n_estimators\": list(np.linspace(200, 1600, 8, dtype=int)),\n",
    "    \"max_features\": [\"auto\", \"sqrt\"],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_depth\": [4, 6, 8, None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ancient-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_params(model,param_dist,X,y):\n",
    "    model_rand = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=100, cv=3, verbose=2, random_state=0, n_jobs=-1)\n",
    "    model_rand.fit(X,y)\n",
    "    best_params = model_rand.best_params_\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "associate-track",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "{'n_estimators': 800, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': None}\n"
     ]
    }
   ],
   "source": [
    "best_params = get_best_params(model,param_dist,X_train,y_train)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-clarity",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "legitimate-optimum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(n_estimators=800)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_best = ExtraTreesClassifier(**best_params)\n",
    "model_best.fit(X_train,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "thousand-filling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9201\n"
     ]
    }
   ],
   "source": [
    "score = cross_val_score(model_best, X_train, y_train)\n",
    "print(np.round(score.mean(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "handed-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance(model,X,y):\n",
    "    y_pred = model.predict(X)\n",
    "    target_names = ['0: No Revenue', '1: Revenue']\n",
    "    report = classification_report(y, y_pred, target_names=target_names)\n",
    "    \n",
    "    report_file = classification_report(y, y_pred, target_names=target_names, output_dict=True)\n",
    "    \n",
    "    print(report_file)\n",
    "    print('\\n')\n",
    "    \n",
    "    report_file = pd.DataFrame(report_file).transpose()\n",
    "    report_file.to_csv(f\"{str(model)}_report1.csv\",index=False)\n",
    "    \n",
    "    print(report)\n",
    "    print(\"Confusion Matrix:\\n\",confusion_matrix(y,y_pred))\n",
    "    print(\"Accuracy: \",accuracy_score(y,y_pred))\n",
    "    print(\"ROC-AUC: \",roc_auc_score(y,model.predict_proba(X)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "geological-biography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0: No Revenue': {'precision': 0.9285506825442927, 'recall': 0.930442374854482, 'f1-score': 0.9294955662160198, 'support': 3436}, '1: Revenue': {'precision': 0.6182108626198083, 'recall': 0.6113744075829384, 'f1-score': 0.6147736298649722, 'support': 633}, 'accuracy': 0.8808060948636028, 'macro avg': {'precision': 0.7733807725820505, 'recall': 0.7709083912187102, 'f1-score': 0.772134598040496, 'support': 4069}, 'weighted avg': {'precision': 0.8802722096978444, 'recall': 0.8808060948636028, 'f1-score': 0.8805353829498086, 'support': 4069}}\n",
      "\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "0: No Revenue       0.93      0.93      0.93      3436\n",
      "   1: Revenue       0.62      0.61      0.61       633\n",
      "\n",
      "     accuracy                           0.88      4069\n",
      "    macro avg       0.77      0.77      0.77      4069\n",
      " weighted avg       0.88      0.88      0.88      4069\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3197  239]\n",
      " [ 246  387]]\n",
      "Accuracy:  0.8808060948636028\n",
      "ROC-AUC:  0.9108103125166669\n"
     ]
    }
   ],
   "source": [
    "get_performance(model_best,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-handy",
   "metadata": {},
   "source": [
    "## Model with Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "intermediate-amateur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "{'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(n_estimators=400)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = get_best_params(model,param_dist,X_train2,y_train)\n",
    "print(best_params)\n",
    "\n",
    "model2 = ExtraTreesClassifier(**best_params)\n",
    "model2.fit(X_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "naked-decision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0: No Revenue': {'precision': 0.9459459459459459, 'recall': 0.9167636786961583, 'f1-score': 0.9311262193319538, 'support': 3436}, '1: Revenue': {'precision': 0.6129905277401895, 'recall': 0.7156398104265402, 'f1-score': 0.6603498542274052, 'support': 633}, 'accuracy': 0.8854755468173998, 'macro avg': {'precision': 0.7794682368430677, 'recall': 0.8162017445613492, 'f1-score': 0.7957380367796796, 'support': 4069}, 'weighted avg': {'precision': 0.8941492441213591, 'recall': 0.8854755468173998, 'f1-score': 0.8890024938192531, 'support': 4069}}\n",
      "\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "0: No Revenue       0.95      0.92      0.93      3436\n",
      "   1: Revenue       0.61      0.72      0.66       633\n",
      "\n",
      "     accuracy                           0.89      4069\n",
      "    macro avg       0.78      0.82      0.80      4069\n",
      " weighted avg       0.89      0.89      0.89      4069\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3150  286]\n",
      " [ 180  453]]\n",
      "Accuracy:  0.8854755468173998\n",
      "ROC-AUC:  0.9266345377537715\n"
     ]
    }
   ],
   "source": [
    "get_performance(model2,X_test2,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-jungle",
   "metadata": {},
   "source": [
    "## Model with Feature Selection and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "unnecessary-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_alt_train.npy', 'rb') as f:\n",
    "     X_alt_train = np.load(f)\n",
    "with open('X_alt_test.npy', 'rb') as f:\n",
    "     X_alt_test = np.load(f)\n",
    "        \n",
    "to_keep = [10, 11, 21, 19, 13, 9, 23, 3, 22, 0, 20, 14, 2, 15, 1, 27, 16, 4, 25, 18]\n",
    "\n",
    "def get_X(to_keep,X_train,X_test,X_alt_train,X_alt_test):\n",
    "    X_train_ss = X_train.iloc[:,to_keep]\n",
    "    X_test_ss = X_test.iloc[:,to_keep]\n",
    "    \n",
    "    X_train3 = np.hstack((X_train_ss.to_numpy(),X_alt_train))\n",
    "    X_test3 = np.hstack((X_test_ss.to_numpy(),X_alt_test))\n",
    "    \n",
    "    return X_train3, X_test3\n",
    "\n",
    "X_train3,X_test3 = get_X(to_keep,X_train,X_test,X_alt_train,X_alt_test)\n",
    "\n",
    "np.save(\"X_trainET.npy\",X_train3)\n",
    "np.save(\"X_testET.npy\",X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adopted-exchange",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "{'n_estimators': 400, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(min_samples_split=5, n_estimators=400)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = get_best_params(model,param_dist,X_train3,y_train)\n",
    "print(best_params)\n",
    "\n",
    "model3 = ExtraTreesClassifier(**best_params)\n",
    "model3.fit(X_train3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "armed-bibliography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0: No Revenue': {'precision': 0.9442771084337349, 'recall': 0.9123981373690337, 'f1-score': 0.9280639431616341, 'support': 3436}, '1: Revenue': {'precision': 0.5981308411214953, 'recall': 0.707740916271722, 'f1-score': 0.6483357452966715, 'support': 633}, 'accuracy': 0.8805603342344557, 'macro avg': {'precision': 0.7712039747776152, 'recall': 0.8100695268203779, 'f1-score': 0.7881998442291528, 'support': 4069}, 'weighted avg': {'precision': 0.8904283526685229, 'recall': 0.8805603342344557, 'f1-score': 0.8845476125525111, 'support': 4069}}\n",
      "\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "0: No Revenue       0.94      0.91      0.93      3436\n",
      "   1: Revenue       0.60      0.71      0.65       633\n",
      "\n",
      "     accuracy                           0.88      4069\n",
      "    macro avg       0.77      0.81      0.79      4069\n",
      " weighted avg       0.89      0.88      0.88      4069\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3135  301]\n",
      " [ 185  448]]\n",
      "Accuracy:  0.8805603342344557\n",
      "ROC-AUC:  0.9227540565741051\n"
     ]
    }
   ],
   "source": [
    "get_performance(model3,X_test3,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
